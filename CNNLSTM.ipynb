{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = \"/home/piyush/Documents/IEMOCAP/test/small_train\"\n",
    "test_data = \"/home/piyush/Documents/IEMOCAP/test/small_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piyush/anaconda2/envs/cs670/lib/python2.7/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(239, 720,...)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# define CNN model\n",
    "model.add(TimeDistributed(Convolution2D(32, 3, 3, input_shape = (239,720,1), activation = 'relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2,2))))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "# # define LSTM model\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# train_datagen = ImageDataGenerator(\n",
    "#         rescale=1./255,\n",
    "#         shear_range=0.2,\n",
    "#         zoom_range=0.2,\n",
    "#         horizontal_flip=True)\n",
    "\n",
    "# test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# train_generator = train_datagen.flow_from_directory('data/train',\n",
    "#                                                     target_size=(239, 720),\n",
    "#                                                     batch_size=32,\n",
    "#                                                     class_mode='categorical')\n",
    "\n",
    "# validation_generator = test_datagen.flow_from_directory('data/validation',\n",
    "#                                                         target_size=(239, 720),\n",
    "#                                                         batch_size=32,\n",
    "#                                                         class_mode='categorical')\n",
    "\n",
    "# model.fit_generator(\n",
    "#         train_generator,\n",
    "#         steps_per_epoch=2000,\n",
    "#         epochs=50,\n",
    "#         validation_data=validation_generator,\n",
    "#         validation_steps=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import subprocess\n",
    "\n",
    "# # os.listdir(\"./\")\n",
    "# # for f in os.listdir(\"./\"):\n",
    "# #     print (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "\n",
    "# from keras.preprocessing import sequence\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Activation\n",
    "# from keras.layers import Embedding\n",
    "# from keras.layers import LSTM\n",
    "# from keras.layers import Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Embedding\n",
    "# max_features = 20000\n",
    "# maxlen = 100\n",
    "# embedding_size = 128\n",
    "\n",
    "# # Convolution\n",
    "# kernel_size = 5\n",
    "# filters = 64\n",
    "# pool_size = 4\n",
    "\n",
    "# # LSTM\n",
    "# lstm_output_size = 70\n",
    "\n",
    "# # Training\n",
    "# batch_size = 30\n",
    "# epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# # define CNN model\n",
    "# model.add(TimeDistributed(Conv2D(...))\n",
    "# model.add(TimeDistributed(MaxPooling2D(...)))\n",
    "# model.add(TimeDistributed(Flatten()))\n",
    "# # define LSTM model\n",
    "# model.add(LSTM(...))\n",
    "# model.add(Dense(...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piyush/anaconda2/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 7s 0us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 103s 4ms/step - loss: 0.3864 - acc: 0.8197 - val_loss: 0.3441 - val_acc: 0.8479\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 104s 4ms/step - loss: 0.1978 - acc: 0.9255 - val_loss: 0.3438 - val_acc: 0.8568\n",
      "25000/25000 [==============================] - 10s 413us/step\n",
      "Test score: 0.34377008080482485\n",
      "Test accuracy: 0.8567999923706054\n"
     ]
    }
   ],
   "source": [
    "# '''Train a recurrent convolutional network on the IMDB sentiment\n",
    "# classification task.\n",
    "\n",
    "# Gets to 0.8498 test accuracy after 2 epochs. 41s/epoch on K520 GPU.\n",
    "# '''\n",
    "# from __future__ import print_function\n",
    "\n",
    "# from keras.preprocessing import sequence\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Activation\n",
    "# from keras.layers import Embedding\n",
    "# from keras.layers import LSTM\n",
    "# from keras.layers import Conv1D, MaxPooling1D\n",
    "# from keras.datasets import imdb\n",
    "\n",
    "# # Embedding\n",
    "# max_features = 20000\n",
    "# maxlen = 100\n",
    "# embedding_size = 128\n",
    "\n",
    "# # Convolution\n",
    "# kernel_size = 5\n",
    "# filters = 64\n",
    "# pool_size = 4\n",
    "\n",
    "# # LSTM\n",
    "# lstm_output_size = 70\n",
    "\n",
    "# # Training\n",
    "# batch_size = 30\n",
    "# epochs = 2\n",
    "\n",
    "# '''\n",
    "# Note:\n",
    "# batch_size is highly sensitive.\n",
    "# Only 2 epochs are needed as the dataset is very small.\n",
    "# '''\n",
    "\n",
    "# print('Loading data...')\n",
    "# (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "# print(len(x_train), 'train sequences')\n",
    "# print(len(x_test), 'test sequences')\n",
    "\n",
    "# print('Pad sequences (samples x time)')\n",
    "# x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "# x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "# print('x_train shape:', x_train.shape)\n",
    "# print('x_test shape:', x_test.shape)\n",
    "\n",
    "# print('Build model...')\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Conv1D(filters,\n",
    "#                  kernel_size,\n",
    "#                  padding='valid',\n",
    "#                  activation='relu',\n",
    "#                  strides=1))\n",
    "# model.add(MaxPooling1D(pool_size=pool_size))\n",
    "# model.add(LSTM(lstm_output_size))\n",
    "# model.add(Dense(1))\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# print('Train...')\n",
    "# model.fit(x_train, y_train,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           validation_data=(x_test, y_test))\n",
    "# score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "# print('Test score:', score)\n",
    "# print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34377008080482485\n"
     ]
    }
   ],
   "source": [
    "# # print (x_train)\n",
    "# print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "import os, subprocess\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import sklearn.metrics as metrics\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = ['Session1', 'Session2', 'Session3', 'Session4', 'Session5']\n",
    "frames_loc = \"/Users/sharingan/Documents/IEMOCAP_vid_frames/\"\n",
    "emotion_classes = ['ang', 'hap', 'neu'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model():\n",
    "    model = Sequential()\n",
    "    # define CNN model\n",
    "    model.add(Conv2D(32, (3, 3), activation = 'relu' ,input_shape = (50,50,1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation = 'relu'))\n",
    "    model.add(Dense(len(emotion_classes), activation = 'softmax'))\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator returns an iterator whose each iteration returns a tuple with following two parts\n",
    "# batch of images of target_size size of selected color_mode ---- in our case 32 images of size 50x50 with 1 channel(grayscale)\n",
    "# ground truth in term of one hot encoding ----- in our case 32 one hot encodings\n",
    "\n",
    "def generate_train(directory):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(directory,\n",
    "                                                        target_size=(50, 50),\n",
    "                                                        batch_size=32,\n",
    "                                                        class_mode='categorical',\n",
    "                                                        color_mode = 'grayscale')\n",
    "    return train_generator\n",
    "\n",
    "def generate_test(directory):\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(directory,\n",
    "                                                            target_size=(50, 50),\n",
    "                                                            batch_size=32,\n",
    "                                                            class_mode='categorical',\n",
    "                                                            color_mode='grayscale')\n",
    "    return test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35437 images belonging to 3 classes.\n",
      "Found 45847 images belonging to 3 classes.\n",
      "Found 40766 images belonging to 3 classes.\n",
      "Found 33323 images belonging to 3 classes.\n",
      "{0: 0.6331805122562504, 1: 1.8142361719269975, 2: 1.1501188070440362}\n",
      "Found 61775 images belonging to 3 classes.\n",
      "Found 45847 images belonging to 3 classes.\n",
      "Found 40766 images belonging to 3 classes.\n",
      "Found 33323 images belonging to 3 classes.\n",
      "{0: 0.635382027854413, 1: 1.7363854408546666, 2: 1.1761458151290964}\n",
      "Found 61775 images belonging to 3 classes.\n",
      "Found 35437 images belonging to 3 classes.\n",
      "Found 40766 images belonging to 3 classes.\n",
      "Found 33323 images belonging to 3 classes.\n",
      "{0: 0.656438849610086, 1: 1.7752870704307093, 2: 1.0948829063762335}\n",
      "Found 61775 images belonging to 3 classes.\n",
      "Found 35437 images belonging to 3 classes.\n",
      "Found 45847 images belonging to 3 classes.\n",
      "Found 33323 images belonging to 3 classes.\n",
      "{0: 0.7167902077440749, 1: 1.6048149361283983, 2: 1.0185717750597691}\n",
      "Found 61775 images belonging to 3 classes.\n",
      "Found 35437 images belonging to 3 classes.\n",
      "Found 45847 images belonging to 3 classes.\n",
      "Found 40766 images belonging to 3 classes.\n",
      "{0: 0.6716761485086652, 1: 1.6901583273569813, 2: 1.0875159733068296}\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=len(sessions))\n",
    "kf = kf.split(sessions)\n",
    "\n",
    "# Calculating class weights\n",
    "for train, test in kf:\n",
    "    y_train = []\n",
    "    for t in train:\n",
    "        train_generator = generate_train(frames_loc + sessions[t])\n",
    "        y_train.extend(train_generator.classes)\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    print class_weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code to print the representation of each class in each session\n",
    "# kf = KFold(n_splits=len(sessions))\n",
    "# kf = kf.split(sessions)\n",
    "# for train, test in kf:\n",
    "#     y_test = []\n",
    "#     for t in test:\n",
    "#         test_generator = generate_test(frames_loc + sessions[t])\n",
    "#         y_test.extend(test_generator.classes)\n",
    "#     print y_test.count(0),\",\",\n",
    "#     print y_test.count(1),\",\",\n",
    "#     print y_test.count(2)\n",
    "# print test_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hap', 'ang', 'neu']\n",
      "{'hap': 1, 'ang': 0, 'neu': 2}\n"
     ]
    }
   ],
   "source": [
    "print test_generator.class_indices.keys()\n",
    "print test_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Found 35437 images belonging to 3 classes.\n",
      "Found 45847 images belonging to 3 classes.\n",
      "Found 40766 images belonging to 3 classes.\n",
      "Found 33323 images belonging to 3 classes.\n",
      "Found 61775 images belonging to 3 classes.\n",
      "Found 35437 images belonging to 3 classes.\n",
      "Epoch 1/5\n",
      "1108/1108 [==============================] - 38s 34ms/step - loss: 0.9377 - acc: 0.5211\n",
      "Epoch 2/5\n",
      "1108/1108 [==============================] - 38s 34ms/step - loss: 0.6199 - acc: 0.7072 1s - loss: 0.622 - ETA: 0s - loss: 0.6211 - a\n",
      "Epoch 3/5\n",
      "1108/1108 [==============================] - 39s 36ms/step - loss: 0.5059 - acc: 0.7630\n",
      "Epoch 4/5\n",
      "1108/1108 [==============================] - 39s 35ms/step - loss: 0.4404 - acc: 0.7940\n",
      "Epoch 5/5\n",
      "1108/1108 [==============================] - 38s 35ms/step - loss: 0.3962 - acc: 0.8181\n",
      "Found 45847 images belonging to 3 classes.\n",
      "Epoch 1/5\n",
      "1433/1433 [==============================] - 49s 34ms/step - loss: 0.5363 - acc: 0.7576\n",
      "Epoch 2/5\n",
      "1433/1433 [==============================] - 50s 35ms/step - loss: 0.3136 - acc: 0.8559\n",
      "Epoch 3/5\n",
      "1433/1433 [==============================] - 51s 35ms/step - loss: 0.2416 - acc: 0.8893\n",
      "Epoch 4/5\n",
      "1433/1433 [==============================] - 51s 36ms/step - loss: 0.2068 - acc: 0.9059\n",
      "Epoch 5/5\n",
      "1433/1433 [==============================] - 52s 37ms/step - loss: 0.1780 - acc: 0.9200\n",
      "Found 40766 images belonging to 3 classes.\n",
      "Epoch 1/5\n",
      "1274/1274 [==============================] - 63s 49ms/step - loss: 0.6784 - acc: 0.7033\n",
      "Epoch 2/5\n",
      "1274/1274 [==============================] - 60s 47ms/step - loss: 0.4636 - acc: 0.7879\n",
      "Epoch 3/5\n",
      "1274/1274 [==============================] - 46s 36ms/step - loss: 0.3914 - acc: 0.8190\n",
      "Epoch 4/5\n",
      "1274/1274 [==============================] - 71s 56ms/step - loss: 0.3458 - acc: 0.8424\n",
      "Epoch 5/5\n",
      "1274/1274 [==============================] - 88s 69ms/step - loss: 0.3216 - acc: 0.8512 \n",
      "Found 33323 images belonging to 3 classes.\n",
      "Epoch 1/5\n",
      "1042/1042 [==============================] - 85s 81ms/step - loss: 0.8424 - acc: 0.6306\n",
      "Epoch 2/5\n",
      " 401/1042 [==========>...................] - ETA: 54s - loss: 0.5257 - acc: 0.7350"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-a52cd0dd820c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;31m#             validation_data=test_generator,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#             validation_steps=np.math.ceil(float(test_generator.samples)/float(test_generator.batch_size))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/ds27/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/ds27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/ds27/lib/python2.7/site-packages/keras/engine/training_generator.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/ds27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/ds27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/ds27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/ds27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create training and testing set\n",
    "\n",
    "kf = KFold(n_splits=len(sessions))\n",
    "kf = kf.split(sessions)\n",
    "\n",
    "# Each iteration represents one fold\n",
    "for train, test in kf:\n",
    "    print \"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\"\n",
    "    print \"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\"\n",
    "    # New model for every fold\n",
    "    model = CNN_model()\n",
    "    \n",
    "    # Calculating class weights\n",
    "    y_train = []\n",
    "    for t in train:\n",
    "        train_generator = generate_train(frames_loc + sessions[t])\n",
    "        y_train.extend(train_generator.classes)\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "    \n",
    "    # Validation data\n",
    "    for t in test:\n",
    "        test_generator = generate_test(frames_loc + sessions[t])\n",
    "    \n",
    "    # Fit training data\n",
    "    for t in train:\n",
    "        train_generator = generate_train(frames_loc + sessions[t])\n",
    "        \n",
    "        model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=np.math.ceil(float(train_generator.samples)/float(train_generator.batch_size)),\n",
    "            class_weight = class_weights,\n",
    "            epochs=5,\n",
    "#             validation_data=test_generator,\n",
    "#             validation_steps=np.math.ceil(float(test_generator.samples)/float(test_generator.batch_size))\n",
    "        )\n",
    "        \n",
    "#     class_labels = list(test_generator.class_indices.keys())\n",
    "    predictions = model.predict_generator(test_generator, np.math.ceil(float(test_generator.samples)/float(test_generator.batch_size)))\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # results\n",
    "    report = metrics.classification_report(test_generator.classes, predicted_classes, target_names=emotion_classes)\n",
    "    \n",
    "    # confusion matrix\n",
    "    confusion_mat = metrics.confusion_matrix(test_generator.classes, predicted_classes)\n",
    "    \n",
    "    print report  \n",
    "    print emotion_classes\n",
    "    print confusion_mat\n",
    "    \n",
    "    with open(\"results.txt\", \"a\") as f:\n",
    "        print >> f, report\n",
    "        print >> f, emotion_classes\n",
    "        print >> f, confusion_mat\n",
    "\n",
    "#     # making directories and storing training and testing samples\n",
    "#     os.system(\"rm -rf train\")\n",
    "#     os.system(\"rm -rf test\")\n",
    "#     os.system(\"mkdir train\")\n",
    "#     os.system(\"mkdir test\")\n",
    "    \n",
    "#     print \"Creating TRAINING SET\"\n",
    "#     dest = \" ./train/\"\n",
    "#     for dr in train:\n",
    "#         src = frames_loc + selected_scripts[dr] + '/'\n",
    "#         print os.system(\"rsync -a \" + src + dest)\n",
    "    \n",
    "#     print \"Creating TESTING SET\"\n",
    "#     dest = \" ./test/\"\n",
    "#     for dr in test:\n",
    "#         src = frames_loc + selected_scripts[dr] + '/'\n",
    "#         print os.system(\"rsync -a \" + src + dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# define CNN model\n",
    "model.add(Conv2D(32, (3, 3), activation = 'relu' ,input_shape = (50,50,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(len(emotion_classes), activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "import os, subprocess\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import sklearn.metrics as metrics\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location where original video is stored and segmented video location\n",
    "segment_loc = \"/Users/sharingan/Documents/IEMOCAP/sentences/avi/\"\n",
    "original_vid_loc = \"/Users/sharingan/Documents/IEMOCAP/avi/DivX/\"\n",
    "\n",
    "# Where you want to store input processed frames\n",
    "frames_loc = \"/Users/sharingan/Documents/IEMOCAP/frames_for_CNN/\"\n",
    "\n",
    "# Location where the datafile is located\n",
    "datafile_loc = \"/Users/sharingan/Documents/IEMOCAP/EmoEvaluation/\"\n",
    "frames_no_stripes_loc = \"/Users/sharingan/Documents/IEMOCAP/Frames_no_stripes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ses01F_script01_1', 'Ses01F_script01_2', 'Ses01F_script01_3', 'Ses01F_script02_1', 'Ses01F_script02_2', 'Ses01F_script03_1', 'Ses01F_script03_2', 'Ses01M_script01_1', 'Ses01M_script01_2', 'Ses01M_script01_3', 'Ses01M_script02_1', 'Ses01M_script02_2', 'Ses01M_script03_1', 'Ses01M_script03_2']\n"
     ]
    }
   ],
   "source": [
    "# Select videos with script (We are not using impro videos)\n",
    "all_vid_names = subprocess.check_output([\"ls\",original_vid_loc]).split()\n",
    "all_names = [x[:-4] for x in all_vid_names]\n",
    "script_vid_names = [x for x in all_names if x.split('_')[1][0] == 's'] \n",
    "print script_vid_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACTING FRAMES AND SAVING THEM IN SCRIPT-WISE FASHION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating directories for frames on the basis of scpritps\n",
    "# creating subdirectories with 4 emotions\n",
    "\n",
    "def create_directory(directory_name):\n",
    "    os.system('mkdir ' + frames_loc + directory_name)\n",
    "\n",
    "# selected_scripts = ['script01', 'script02', 'script03']\n",
    "selected_scripts = ['test', 'test2']\n",
    "# selected_emotions = ['ang', 'hap', 'sad', 'neu']\n",
    "selected_emotions = ['ang', 'neu']\n",
    "for script in selected_scripts:\n",
    "    create_directory(script)\n",
    "    for emo in selected_emotions:\n",
    "        create_directory(script + \"/\" + emo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only needed to execute once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_videos_for_CNN(video_input_file_path, script, emo, segment_name):\n",
    "#     print '\\n Extracting frames from video: ', video_input_file_path\n",
    "#     vidcap = cv2.VideoCapture(video_input_file_path)\n",
    "#     success, image = vidcap.read()\n",
    "#     features = []\n",
    "#     success = True\n",
    "#     count = 0\n",
    "#     while success:\n",
    "# #         plt.imshow(image, cmap='gray')\n",
    "# #         plt.xticks([]),plt.yticks([])  # to hide tick values on X and Y axis\n",
    "# #         plt.show()\n",
    "#         success, image = vidcap.read()\n",
    "# #         print('Read a new frame: ', success)\n",
    "#         if success:\n",
    "#             image = image[120:360, 0:720]\n",
    "#             image = cv2.resize(image, (480, 480), interpolation=cv2.INTER_AREA)\n",
    "#             image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# #             channels = image.shape[2]\n",
    "# #             print image.shape\n",
    "# #             for channel in range(channels):\n",
    "# #                 features.append(image[:, :, channel])\n",
    "#             count = count + 1\n",
    "#             loc = frames_loc + script + '/' + emo + '/' + segment_name + '_' + str(count) + '.jpg'\n",
    "#             print cv2.imwrite(loc, image),\n",
    "        \n",
    "\n",
    "# for name in script_vid_names:\n",
    "# #     print \"\\n Extracting frames from \", name\n",
    "#     txt_file_name = name + '.txt'\n",
    "#     with open(datafile_loc + txt_file_name) as f:\n",
    "#         data = f.readlines()\n",
    "#     data = iter(data)\n",
    "#     try:\n",
    "#         for line in data:\n",
    "#             if line != '\\n':\n",
    "#                 continue;\n",
    "#             line = next(data)\n",
    "#             segment_name = line.split()[3]\n",
    "#             emotion = line.split()[4]\n",
    "#             script = segment_name.split('_')[1]\n",
    "#             this_segment_loc = segment_loc + name + '/' + segment_name + '.avi'\n",
    "#             if emotion in selected_emotions:\n",
    "#                 extract_videos_for_CNN(this_segment_loc, script, emotion, segment_name)\n",
    "            \n",
    "#     except(StopIteration):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch size\n",
    "bs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model():\n",
    "    model = Sequential()\n",
    "    # define CNN model\n",
    "    model.add(Conv2D(bs, (3, 3), activation = 'relu' ,input_shape = (480,480,1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation = 'relu'))\n",
    "    model.add(Dense(len(selected_emotions), activation = 'softmax'))\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return model\n",
    "# print model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train(directory):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(directory,\n",
    "                                                        target_size=(480, 480),\n",
    "                                                        batch_size=bs,\n",
    "                                                        class_mode='categorical',\n",
    "                                                        color_mode = 'grayscale')\n",
    "    return train_generator\n",
    "\n",
    "def generate_test(directory):\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(directory,\n",
    "                                                            target_size=(480, 480),\n",
    "                                                            batch_size=bs,\n",
    "                                                            class_mode='categorical',\n",
    "                                                            color_mode='grayscale')\n",
    "    return test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 149 images belonging to 2 classes.\n",
      "{0: 0.8465909090909091, 1: 1.221311475409836}\n",
      "Found 231 images belonging to 2 classes.\n",
      "{0: 1.375, 1: 0.7857142857142857}\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=len(selected_scripts))\n",
    "kf = kf.split(selected_scripts)\n",
    "\n",
    "# Calculating class weights\n",
    "for train, test in kf:\n",
    "    y_train = []\n",
    "    for t in train:\n",
    "        train_generator = generate_train(frames_loc + selected_scripts[t])\n",
    "        y_train.extend(train_generator.classes)\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    print class_weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Found 149 images belonging to 2 classes.\n",
      "Found 231 images belonging to 2 classes.\n",
      "Found 149 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "149/149 [==============================] - 13s 90ms/step - loss: 8.0617 - acc: 0.5906 - val_loss: 10.2570 - val_acc: 0.3636\n",
      "Epoch 2/5\n",
      "149/149 [==============================] - 13s 85ms/step - loss: 8.0590 - acc: 0.5906 - val_loss: 10.2570 - val_acc: 0.3636\n",
      "Epoch 3/5\n",
      "149/149 [==============================] - 13s 86ms/step - loss: 8.0590 - acc: 0.5906 - val_loss: 10.2570 - val_acc: 0.3636\n",
      "Epoch 4/5\n",
      "149/149 [==============================] - 13s 87ms/step - loss: 8.0590 - acc: 0.5906 - val_loss: 10.2570 - val_acc: 0.3636\n",
      "Epoch 5/5\n",
      "149/149 [==============================] - 13s 86ms/step - loss: 8.0590 - acc: 0.5906 - val_loss: 10.2570 - val_acc: 0.3636\n",
      "validation result:\n",
      "0.36363636363636365\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang       0.36      1.00      0.53        84\n",
      "         neu       0.00      0.00      0.00       147\n",
      "\n",
      "   micro avg       0.36      0.36      0.36       231\n",
      "   macro avg       0.18      0.50      0.27       231\n",
      "weighted avg       0.13      0.36      0.19       231\n",
      "\n",
      "['ang', 'neu']\n",
      "[[ 84   0]\n",
      " [147   0]]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Found 231 images belonging to 2 classes.\n",
      "Found 149 images belonging to 2 classes.\n",
      "Found 231 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "231/231 [==============================] - 19s 81ms/step - loss: 0.6966 - acc: 0.4935 - val_loss: 0.6931 - val_acc: 0.5906\n",
      "Epoch 2/5\n",
      "231/231 [==============================] - 21s 93ms/step - loss: 0.6936 - acc: 0.4329 - val_loss: 0.6928 - val_acc: 0.5906\n",
      "Epoch 3/5\n",
      " 13/231 [>.............................] - ETA: 23s - loss: 0.6707 - acc: 0.3077"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8fb0d4ca5d11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             validation_steps=test_generator.samples / test_generator.batch_size)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"validation result:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/ds27/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/ds27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/ds27/lib/python2.7/site-packages/keras/engine/training_generator.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/ds27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/ds27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/ds27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/ds27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create training and testing set\n",
    "\n",
    "kf = KFold(n_splits=len(selected_scripts))\n",
    "kf = kf.split(selected_scripts)\n",
    "\n",
    "# Each iteration represents one fold\n",
    "for train, test in kf:\n",
    "    print \"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\"\n",
    "    print \"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\"\n",
    "    # New model for every fold\n",
    "    model = CNN_model()\n",
    "    \n",
    "    # Calculating class weights\n",
    "    y_train = []\n",
    "    for t in train:\n",
    "        train_generator = generate_train(frames_loc + selected_scripts[t])\n",
    "        y_train.extend(train_generator.classes)\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "    \n",
    "    # Validation data\n",
    "    for t in test:\n",
    "        test_generator = generate_test(frames_loc + selected_scripts[t])\n",
    "    \n",
    "    # Fit training data\n",
    "    for t in train:\n",
    "        train_generator = generate_train(frames_loc + selected_scripts[t])\n",
    "        \n",
    "        model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=train_generator.samples / train_generator.batch_size,\n",
    "            class_weight = class_weights,\n",
    "            epochs=5,\n",
    "            validation_data=test_generator,\n",
    "            validation_steps=test_generator.samples / test_generator.batch_size)\n",
    "    \n",
    "    print \"validation result:\"\n",
    "    print np.mean(model.history.history['val_acc'])\n",
    "        \n",
    "    class_labels = list(test_generator.class_indices.keys())\n",
    "    predictions = model.predict_generator(test_generator, np.math.ceil(float(test_generator.samples)/float(test_generator.batch_size)))\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # results\n",
    "    report = metrics.classification_report(test_generator.classes, predicted_classes, target_names=class_labels)\n",
    "    \n",
    "    # confusion matrix\n",
    "    confusion_mat = metrics.confusion_matrix(test_generator.classes, predicted_classes)\n",
    "    \n",
    "    print report  \n",
    "    print class_labels\n",
    "    print confusion_mat\n",
    "\n",
    "#     # making directories and storing training and testing samples\n",
    "#     os.system(\"rm -rf train\")\n",
    "#     os.system(\"rm -rf test\")\n",
    "#     os.system(\"mkdir train\")\n",
    "#     os.system(\"mkdir test\")\n",
    "    \n",
    "#     print \"Creating TRAINING SET\"\n",
    "#     dest = \" ./train/\"\n",
    "#     for dr in train:\n",
    "#         src = frames_loc + selected_scripts[dr] + '/'\n",
    "#         print os.system(\"rsync -a \" + src + dest)\n",
    "    \n",
    "#     print \"Creating TESTING SET\"\n",
    "#     dest = \" ./test/\"\n",
    "#     for dr in test:\n",
    "#         src = frames_loc + selected_scripts[dr] + '/'\n",
    "#         print os.system(\"rsync -a \" + src + dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4093959731543625\n"
     ]
    }
   ],
   "source": [
    "print np.mean(model.history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "import os, subprocess\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import sklearn.metrics as metrics\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location where original video is stored and segmented video location\n",
    "segment_loc = \"/Users/sharingan/Documents/IEMOCAP/sentences/avi/\"\n",
    "original_vid_loc = \"/Users/sharingan/Documents/IEMOCAP/avi/DivX/\"\n",
    "\n",
    "# Where you want to store input processed frames\n",
    "frames_loc = \"/Users/sharingan/Documents/IEMOCAP/frames_for_CNN/\"\n",
    "\n",
    "# Location where the datafile is located\n",
    "datafile_loc = \"/Users/sharingan/Documents/IEMOCAP/EmoEvaluation/\"\n",
    "frames_no_stripes_loc = \"/Users/sharingan/Documents/IEMOCAP/Frames_no_stripes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ses01F_script01_1', 'Ses01F_script01_2', 'Ses01F_script01_3', 'Ses01F_script02_1', 'Ses01F_script02_2', 'Ses01F_script03_1', 'Ses01F_script03_2', 'Ses01M_script01_1', 'Ses01M_script01_2', 'Ses01M_script01_3', 'Ses01M_script02_1', 'Ses01M_script02_2', 'Ses01M_script03_1', 'Ses01M_script03_2']\n"
     ]
    }
   ],
   "source": [
    "# Select videos with script (We are not using impro videos)\n",
    "all_vid_names = subprocess.check_output([\"ls\",original_vid_loc]).split()\n",
    "all_names = [x[:-4] for x in all_vid_names]\n",
    "script_vid_names = [x for x in all_names if x.split('_')[1][0] == 's'] \n",
    "print script_vid_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACTING FRAMES AND SAVING THEM IN SCRIPT-WISE FASHION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating directories for frames on the basis of scpritps\n",
    "# creating subdirectories with 4 emotions\n",
    "\n",
    "def create_directory(directory_name):\n",
    "    os.system('mkdir ' + frames_loc + directory_name)\n",
    "\n",
    "# selected_scripts = ['script01', 'script02', 'script03']\n",
    "selected_scripts = ['test', 'test2', 'test3']\n",
    "# selected_emotions = ['ang', 'hap', 'sad', 'neu']\n",
    "selected_emotions = ['ang', 'neu']\n",
    "for script in selected_scripts:\n",
    "    create_directory(script)\n",
    "    for emo in selected_emotions:\n",
    "        create_directory(script + \"/\" + emo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only needed to execute once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_videos_for_CNN(video_input_file_path, script, emo, segment_name):\n",
    "#     print '\\n Extracting frames from video: ', video_input_file_path\n",
    "#     vidcap = cv2.VideoCapture(video_input_file_path)\n",
    "#     success, image = vidcap.read()\n",
    "#     features = []\n",
    "#     success = True\n",
    "#     count = 0\n",
    "#     while success:\n",
    "# #         plt.imshow(image, cmap='gray')\n",
    "# #         plt.xticks([]),plt.yticks([])  # to hide tick values on X and Y axis\n",
    "# #         plt.show()\n",
    "#         success, image = vidcap.read()\n",
    "# #         print('Read a new frame: ', success)\n",
    "#         if success:\n",
    "#             image = image[120:360, 0:720]\n",
    "#             image = cv2.resize(image, (480, 480), interpolation=cv2.INTER_AREA)\n",
    "#             image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# #             channels = image.shape[2]\n",
    "# #             print image.shape\n",
    "# #             for channel in range(channels):\n",
    "# #                 features.append(image[:, :, channel])\n",
    "#             count = count + 1\n",
    "#             loc = frames_loc + script + '/' + emo + '/' + segment_name + '_' + str(count) + '.jpg'\n",
    "#             print cv2.imwrite(loc, image),\n",
    "        \n",
    "\n",
    "# for name in script_vid_names:\n",
    "# #     print \"\\n Extracting frames from \", name\n",
    "#     txt_file_name = name + '.txt'\n",
    "#     with open(datafile_loc + txt_file_name) as f:\n",
    "#         data = f.readlines()\n",
    "#     data = iter(data)\n",
    "#     try:\n",
    "#         for line in data:\n",
    "#             if line != '\\n':\n",
    "#                 continue;\n",
    "#             line = next(data)\n",
    "#             segment_name = line.split()[3]\n",
    "#             emotion = line.split()[4]\n",
    "#             script = segment_name.split('_')[1]\n",
    "#             this_segment_loc = segment_loc + name + '/' + segment_name + '.avi'\n",
    "#             if emotion in selected_emotions:\n",
    "#                 extract_videos_for_CNN(this_segment_loc, script, emotion, segment_name)\n",
    "            \n",
    "#     except(StopIteration):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch size\n",
    "bs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model():\n",
    "    model = Sequential()\n",
    "    # define CNN model\n",
    "    model.add(Conv2D(bs, (3, 3), activation = 'relu' ,input_shape = (50,50,1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation = 'relu'))\n",
    "    model.add(Dense(len(selected_emotions), activation = 'softmax'))\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return model\n",
    "# print model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train(directory):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(directory,\n",
    "                                                        target_size=(50, 50),\n",
    "                                                        batch_size=bs,\n",
    "                                                        class_mode='categorical',\n",
    "                                                        color_mode = 'grayscale')\n",
    "    return train_generator\n",
    "\n",
    "def generate_test(directory):\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(directory,\n",
    "                                                            target_size=(50, 50),\n",
    "                                                            batch_size=bs,\n",
    "                                                            class_mode='categorical',\n",
    "                                                            color_mode='grayscale')\n",
    "    return test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 645 images belonging to 2 classes.\n",
      "Found 1048 images belonging to 2 classes.\n",
      "{0: 1.1347184986595173, 1: 0.8938753959873285}\n",
      "Found 683 images belonging to 2 classes.\n",
      "Found 1048 images belonging to 2 classes.\n",
      "{0: 1.2879464285714286, 1: 0.8172804532577904}\n",
      "Found 683 images belonging to 2 classes.\n",
      "Found 645 images belonging to 2 classes.\n",
      "{0: 1.0849673202614378, 1: 0.9273743016759777}\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=len(selected_scripts))\n",
    "kf = kf.split(selected_scripts)\n",
    "\n",
    "# Calculating class weights\n",
    "for train, test in kf:\n",
    "    y_train = []\n",
    "    for t in train:\n",
    "        train_generator = generate_train(frames_loc + selected_scripts[t])\n",
    "        y_train.extend(train_generator.classes)\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    print class_weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Found 645 images belonging to 2 classes.\n",
      "Found 1048 images belonging to 2 classes.\n",
      "Found 683 images belonging to 2 classes.\n",
      "Found 645 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.7105 - acc: 0.5262 - val_loss: 0.7287 - val_acc: 0.3939\n",
      "Epoch 2/10\n",
      "162/162 [==============================] - 4s 27ms/step - loss: 0.5565 - acc: 0.6867 - val_loss: 0.6861 - val_acc: 0.6061\n",
      "Epoch 3/10\n",
      "162/162 [==============================] - 4s 27ms/step - loss: 0.3369 - acc: 0.9306 - val_loss: 0.6738 - val_acc: 0.6061\n",
      "Epoch 4/10\n",
      "162/162 [==============================] - 4s 27ms/step - loss: 0.2470 - acc: 0.9784 - val_loss: 0.6708 - val_acc: 0.6061\n",
      "Epoch 5/10\n",
      "162/162 [==============================] - 4s 27ms/step - loss: 0.1995 - acc: 0.9938 - val_loss: 0.6786 - val_acc: 0.6061\n",
      "Epoch 6/10\n",
      "162/162 [==============================] - 4s 26ms/step - loss: 0.1617 - acc: 1.0000 - val_loss: 0.6955 - val_acc: 0.6061\n",
      "Epoch 7/10\n",
      "162/162 [==============================] - 4s 27ms/step - loss: 0.1398 - acc: 0.9985 - val_loss: 0.7191 - val_acc: 0.6061\n",
      "Epoch 8/10\n",
      "162/162 [==============================] - 4s 27ms/step - loss: 0.1162 - acc: 0.9985 - val_loss: 0.7472 - val_acc: 0.6061\n",
      "Epoch 9/10\n",
      "162/162 [==============================] - 4s 27ms/step - loss: 0.0981 - acc: 1.0000 - val_loss: 0.7787 - val_acc: 0.6061\n",
      "Epoch 10/10\n",
      "162/162 [==============================] - 4s 27ms/step - loss: 0.0855 - acc: 1.0000 - val_loss: 0.8124 - val_acc: 0.6061\n",
      "Found 1048 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "262/262 [==============================] - 6s 24ms/step - loss: 0.8057 - acc: 0.6135 - val_loss: 0.7085 - val_acc: 0.6076\n",
      "Epoch 2/10\n",
      "262/262 [==============================] - 6s 23ms/step - loss: 0.7234 - acc: 0.6155 - val_loss: 0.6768 - val_acc: 0.6091\n",
      "Epoch 3/10\n",
      "262/262 [==============================] - 6s 23ms/step - loss: 0.6942 - acc: 0.6155 - val_loss: 0.6720 - val_acc: 0.6047\n",
      "Epoch 4/10\n",
      "262/262 [==============================] - 6s 23ms/step - loss: 0.6835 - acc: 0.6155 - val_loss: 0.6705 - val_acc: 0.6061\n",
      "Epoch 5/10\n",
      "262/262 [==============================] - 6s 23ms/step - loss: 0.6796 - acc: 0.6155 - val_loss: 0.6722 - val_acc: 0.6047\n",
      "Epoch 6/10\n",
      "262/262 [==============================] - 6s 23ms/step - loss: 0.6781 - acc: 0.6155 - val_loss: 0.6729 - val_acc: 0.6061\n",
      "Epoch 7/10\n",
      "262/262 [==============================] - 6s 22ms/step - loss: 0.6777 - acc: 0.6155 - val_loss: 0.6728 - val_acc: 0.6091\n",
      "Epoch 8/10\n",
      "262/262 [==============================] - 6s 23ms/step - loss: 0.6776 - acc: 0.6155 - val_loss: 0.6740 - val_acc: 0.6061\n",
      "Epoch 9/10\n",
      "262/262 [==============================] - 6s 23ms/step - loss: 0.6776 - acc: 0.6155 - val_loss: 0.6752 - val_acc: 0.6018\n",
      "Epoch 10/10\n",
      "262/262 [==============================] - 6s 23ms/step - loss: 0.6774 - acc: 0.6155 - val_loss: 0.6741 - val_acc: 0.6091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang       0.00      0.00      0.00       269\n",
      "         neu       0.61      1.00      0.75       414\n",
      "\n",
      "   micro avg       0.61      0.61      0.61       683\n",
      "   macro avg       0.30      0.50      0.38       683\n",
      "weighted avg       0.37      0.61      0.46       683\n",
      "\n",
      "['ang', 'neu']\n",
      "[[  0 269]\n",
      " [  0 414]]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Found 683 images belonging to 2 classes.\n",
      "Found 1048 images belonging to 2 classes.\n",
      "Found 645 images belonging to 2 classes.\n",
      "Found 683 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.6315 - acc: 0.6798 - val_loss: 0.8321 - val_acc: 0.4140\n",
      "Epoch 2/10\n",
      "171/171 [==============================] - 4s 26ms/step - loss: 0.2313 - acc: 0.9547 - val_loss: 1.2380 - val_acc: 0.2233\n",
      "Epoch 3/10\n",
      "171/171 [==============================] - 4s 26ms/step - loss: 0.0687 - acc: 0.9971 - val_loss: 1.6264 - val_acc: 0.5287\n",
      "Epoch 4/10\n",
      "171/171 [==============================] - 4s 26ms/step - loss: 0.0254 - acc: 1.0000 - val_loss: 1.9174 - val_acc: 0.4775\n",
      "Epoch 5/10\n",
      "171/171 [==============================] - 4s 26ms/step - loss: 0.0138 - acc: 1.0000 - val_loss: 1.9841 - val_acc: 0.4558\n",
      "Epoch 6/10\n",
      "171/171 [==============================] - 4s 25ms/step - loss: 0.0066 - acc: 1.0000 - val_loss: 2.1346 - val_acc: 0.4558\n",
      "Epoch 7/10\n",
      "171/171 [==============================] - 4s 25ms/step - loss: 0.0078 - acc: 1.0000 - val_loss: 2.3797 - val_acc: 0.5318\n",
      "Epoch 8/10\n",
      "171/171 [==============================] - 4s 26ms/step - loss: 0.0046 - acc: 1.0000 - val_loss: 2.1442 - val_acc: 0.4512\n",
      "Epoch 9/10\n",
      "171/171 [==============================] - 4s 26ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 2.4392 - val_acc: 0.5147\n",
      "Epoch 10/10\n",
      "171/171 [==============================] - 4s 25ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 2.5631 - val_acc: 0.5271\n",
      "Found 1048 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "262/262 [==============================] - 6s 23ms/step - loss: 0.7458 - acc: 0.5849 - val_loss: 0.5137 - val_acc: 0.9070\n",
      "Epoch 2/10\n",
      "262/262 [==============================] - 6s 22ms/step - loss: 0.3872 - acc: 0.8664 - val_loss: 0.0736 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "262/262 [==============================] - 6s 23ms/step - loss: 0.1649 - acc: 0.9494 - val_loss: 0.0680 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "262/262 [==============================] - 6s 23ms/step - loss: 0.0928 - acc: 0.9819 - val_loss: 0.0335 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "262/262 [==============================] - 6s 23ms/step - loss: 0.0526 - acc: 0.9933 - val_loss: 0.0325 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "262/262 [==============================] - 6s 22ms/step - loss: 0.0395 - acc: 0.9933 - val_loss: 0.0556 - val_acc: 0.9938\n",
      "Epoch 7/10\n",
      "262/262 [==============================] - 6s 23ms/step - loss: 0.0290 - acc: 0.9943 - val_loss: 0.0663 - val_acc: 0.9922\n",
      "Epoch 8/10\n",
      "262/262 [==============================] - 6s 22ms/step - loss: 0.0216 - acc: 0.9981 - val_loss: 0.0170 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "262/262 [==============================] - 6s 22ms/step - loss: 0.0270 - acc: 0.9933 - val_loss: 0.0569 - val_acc: 0.9876\n",
      "Epoch 10/10\n",
      "262/262 [==============================] - 6s 22ms/step - loss: 0.0202 - acc: 0.9971 - val_loss: 0.0508 - val_acc: 0.9907\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang       0.53      0.54      0.54       343\n",
      "         neu       0.47      0.46      0.47       302\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       645\n",
      "   macro avg       0.50      0.50      0.50       645\n",
      "weighted avg       0.50      0.51      0.51       645\n",
      "\n",
      "['ang', 'neu']\n",
      "[[186 157]\n",
      " [162 140]]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Found 683 images belonging to 2 classes.\n",
      "Found 645 images belonging to 2 classes.\n",
      "Found 1048 images belonging to 2 classes.\n",
      "Found 683 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "171/171 [==============================] - 7s 42ms/step - loss: 0.6452 - acc: 0.6686 - val_loss: 0.6664 - val_acc: 0.6155\n",
      "Epoch 2/10\n",
      "171/171 [==============================] - 6s 33ms/step - loss: 0.3571 - acc: 0.8743 - val_loss: 1.1645 - val_acc: 0.6155\n",
      "Epoch 3/10\n",
      "171/171 [==============================] - 6s 33ms/step - loss: 0.1046 - acc: 0.9766 - val_loss: 1.1587 - val_acc: 0.6155\n",
      "Epoch 4/10\n",
      "171/171 [==============================] - 6s 33ms/step - loss: 0.0204 - acc: 1.0000 - val_loss: 1.0001 - val_acc: 0.6155\n",
      "Epoch 5/10\n",
      "171/171 [==============================] - 6s 33ms/step - loss: 0.0092 - acc: 1.0000 - val_loss: 1.6940 - val_acc: 0.6155\n",
      "Epoch 6/10\n",
      "171/171 [==============================] - 6s 33ms/step - loss: 0.0062 - acc: 1.0000 - val_loss: 1.3377 - val_acc: 0.6155\n",
      "Epoch 7/10\n",
      "171/171 [==============================] - 6s 34ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 1.6197 - val_acc: 0.6155\n",
      "Epoch 8/10\n",
      "171/171 [==============================] - 6s 33ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 1.5444 - val_acc: 0.6155\n",
      "Epoch 9/10\n",
      "171/171 [==============================] - 6s 33ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 1.7778 - val_acc: 0.6155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "171/171 [==============================] - 6s 33ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 2.0524 - val_acc: 0.6155\n",
      "Found 645 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1804 - acc: 0.9475 - val_loss: 3.0923 - val_acc: 0.6164\n",
      "Epoch 2/10\n",
      "162/162 [==============================] - 6s 34ms/step - loss: 0.0077 - acc: 1.0000 - val_loss: 4.1297 - val_acc: 0.6164\n",
      "Epoch 3/10\n",
      "162/162 [==============================] - 5s 34ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 4.1050 - val_acc: 0.6145\n",
      "Epoch 4/10\n",
      "162/162 [==============================] - 5s 34ms/step - loss: 8.9479e-04 - acc: 1.0000 - val_loss: 4.2958 - val_acc: 0.6145\n",
      "Epoch 5/10\n",
      "162/162 [==============================] - 6s 34ms/step - loss: 7.1749e-04 - acc: 1.0000 - val_loss: 3.9921 - val_acc: 0.6164\n",
      "Epoch 6/10\n",
      "162/162 [==============================] - 6s 34ms/step - loss: 4.6168e-04 - acc: 1.0000 - val_loss: 4.5840 - val_acc: 0.6126\n",
      "Epoch 7/10\n",
      "162/162 [==============================] - 5s 34ms/step - loss: 4.7944e-04 - acc: 1.0000 - val_loss: 5.0256 - val_acc: 0.6183\n",
      "Epoch 8/10\n",
      "162/162 [==============================] - 6s 34ms/step - loss: 2.7210e-04 - acc: 1.0000 - val_loss: 4.7060 - val_acc: 0.6135\n",
      "Epoch 9/10\n",
      "162/162 [==============================] - 6s 35ms/step - loss: 1.6757e-04 - acc: 1.0000 - val_loss: 4.7290 - val_acc: 0.6155\n",
      "Epoch 10/10\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 2.0221e-04 - acc: 1.0000 - val_loss: 4.9264 - val_acc: 0.6164\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang       0.00      0.00      0.00       403\n",
      "         neu       0.62      1.00      0.76       645\n",
      "\n",
      "   micro avg       0.62      0.62      0.62      1048\n",
      "   macro avg       0.31      0.50      0.38      1048\n",
      "weighted avg       0.38      0.62      0.47      1048\n",
      "\n",
      "['ang', 'neu']\n",
      "[[  0 403]\n",
      " [  0 645]]\n"
     ]
    }
   ],
   "source": [
    "# create training and testing set\n",
    "\n",
    "kf = KFold(n_splits=len(selected_scripts))\n",
    "kf = kf.split(selected_scripts)\n",
    "\n",
    "# Each iteration represents one fold\n",
    "for train, test in kf:\n",
    "    print \"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\"\n",
    "    print \"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\"\n",
    "    # New model for every fold\n",
    "    model = CNN_model()\n",
    "    \n",
    "    # Calculating class weights\n",
    "    y_train = []\n",
    "    for t in train:\n",
    "        train_generator = generate_train(frames_loc + selected_scripts[t])\n",
    "        y_train.extend(train_generator.classes)\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "    \n",
    "    # Validation data\n",
    "    for t in test:\n",
    "        test_generator = generate_test(frames_loc + selected_scripts[t])\n",
    "    \n",
    "    # Fit training data\n",
    "    for t in train:\n",
    "        train_generator = generate_train(frames_loc + selected_scripts[t])\n",
    "        \n",
    "        model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=np.math.ceil(float(train_generator.samples)/float(train_generator.batch_size)),\n",
    "            class_weight = class_weights,\n",
    "            epochs=10,\n",
    "            validation_data=test_generator,\n",
    "            validation_steps=np.math.ceil(float(test_generator.samples)/float(test_generator.batch_size)))\n",
    "        \n",
    "    class_labels = list(test_generator.class_indices.keys())\n",
    "    predictions = model.predict_generator(test_generator, np.math.ceil(float(test_generator.samples)/float(test_generator.batch_size)))\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # results\n",
    "    report = metrics.classification_report(test_generator.classes, predicted_classes, target_names=class_labels)\n",
    "    \n",
    "    # confusion matrix\n",
    "    confusion_mat = metrics.confusion_matrix(test_generator.classes, predicted_classes)\n",
    "    \n",
    "    print report  \n",
    "    print class_labels\n",
    "    print confusion_mat\n",
    "    \n",
    "    with open(\"results.txt\", \"a\") as f:\n",
    "        print >> f, report\n",
    "        print >> f, class_labels\n",
    "        print >> f, confusion_mat\n",
    "\n",
    "#     # making directories and storing training and testing samples\n",
    "#     os.system(\"rm -rf train\")\n",
    "#     os.system(\"rm -rf test\")\n",
    "#     os.system(\"mkdir train\")\n",
    "#     os.system(\"mkdir test\")\n",
    "    \n",
    "#     print \"Creating TRAINING SET\"\n",
    "#     dest = \" ./train/\"\n",
    "#     for dr in train:\n",
    "#         src = frames_loc + selected_scripts[dr] + '/'\n",
    "#         print os.system(\"rsync -a \" + src + dest)\n",
    "    \n",
    "#     print \"Creating TESTING SET\"\n",
    "#     dest = \" ./test/\"\n",
    "#     for dr in test:\n",
    "#         src = frames_loc + selected_scripts[dr] + '/'\n",
    "#         print os.system(\"rsync -a \" + src + dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 50, 50, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, y = next(train_generator)\n",
    "print img.shape\n",
    "img = (img[0]).reshape((50, 50))\n",
    "\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.xticks([]),plt.yticks([])  # to hide tick values on X and Y axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
